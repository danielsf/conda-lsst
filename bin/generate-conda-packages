#!/usr/bin/env python

import os, os.path, shutil, subprocess, re, sys, glob, tempfile
from collections import OrderedDict, namedtuple
import requests, sqlalchemy

def conda_name_for(product):
	# return the conda package name for a product
	try:
		return eups_to_conda_map[product]
	except KeyError:
		pass

	transformed_name = product.replace('_', '-')

	if product in dont_prefix_products:
		return transformed_name
	else:
		return lsst_prefix + transformed_name

# Output directory where the package specs will be generated (and the rebuild script)
# DANGER, DANGER: Be careful what you set this to -- it will be 'rm -rf'-ed !!!
output_dir = "recipes/generated"

# Products that already exist in Anaconda; we'll skip building those (but will depend on them)
internal_products = set("python swig libevent flask twisted scons numpy protobuf matplotlib".split())

# Products to skip alltogether (i.e., don't build, don't make a dependency)
skip_products = set("anaconda afwdata sims_GalSimInterface GalSim".split())

# Products that need to be prefixed with our prefix to avoid collisions
# Products whose Conda name will _not_ be prefixed with out namespace prefix
dont_prefix_products = set("legacy_configs".split()) | internal_products
lsst_prefix="lsst-"

# A specific mapping between an EUPS product name and Conda product name. Takes
# precedence over automatic prefixing.
eups_to_conda_map = {
	'legacy_configs':	'legacy_configs',
	'lsst':			lsst_prefix + 'eups-environment',
	'lsst_sims':		lsst_prefix + 'sims',
	'lsst_distrib':		lsst_prefix + 'distrib',
	'lsst_apps':		lsst_prefix + 'apps',
}

# Missing dependencies (these would be transparently installed with pip otherwise)
missing_deps = { # map of conda_name -> [ (pkgtype, conda_name), ... ]
	conda_name_for('pymssql')                  : [('conda', 'cython'), ('pypi', 'setuptools-git')],
	conda_name_for('palpy')                    : [('conda', 'cython'), ('conda', 'numpy')],
	conda_name_for('pyfits')                   : [('pypi', 'stsci.distutils')],
	conda_name_for('sims_catalogs_generation') : [('conda', 'sqlalchemy')],
	conda_name_for('sims_photUtils')           : [('conda', 'scipy'), ('conda', 'astropy')],

	'stsci.distutils'          : [('pypi', 'd2to1')],	# needed by pyfits
}

# Parsers for versions that cannot be parsed otherwise
special_version_parsers = { # map of eups_product_name -> [ parser_function1, parser_function2, ... ]
}

# The EUPS tags to apply to all products build in this run
# You should always leave 'current' (unless you know what you're doing)
# You should also leave 'conda', to allow the user to see right away that this
# is a Conda-installed and managed EUPS product
eups_tags = [ 'current', 'conda' ]

# Override sha1s -- these are temporary hacks until the fixes below get merged
override_gitrev = {
#	'sconsUtils':    'u/mjuric/osx-deployment-target',		# Now handled via patch
#	'webservcommon': 'u/mjuric/DM-2993-never-depend-on-anaconda',	# Handle internally with skip_products
#	'healpy':        'u/mjuric/sanitize-flags',			# Apply patch
#	'log':           'u/mjuric/DM-2995-fix-EINTR-in-testcase',	# Handle internally by not failing on compileall
#	'ctrl_events':   'u/mjuric/osx-compatibility',			# Fixed on master
#	'ctrl_orca':     'u/mjuric/typo-fix-in-production_data'		# Handle internally by not failing on compileall
}

ProductInfo = namedtuple('ProductInfo', ['conda_name', 'version', 'build_string', 'buildnum', 'product', 'eups_version', 'deps'])

# Cribbed from http://stackoverflow.com/questions/16694907/how-to-download-large-file-in-python-with-requests-py
def download_url(url, fp):
	# Download the contents of url into a file-like object fp
	r = requests.get(url, stream=True)
	r.raise_for_status()

	for chunk in r.iter_content(chunk_size=10*1024*1024): 
		if chunk: # filter out keep-alive new chunks
			fp.write(chunk)
	fp.flush()

from sqlalchemy import Column, Integer, String, ForeignKey, UniqueConstraint
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, backref
from sqlalchemy.orm import sessionmaker, make_transient

Base = declarative_base()
class Channel(Base):
	__tablename__  = 'channels'
	__table_args__ = {'sqlite_autoincrement': True}

	id       = Column(Integer, primary_key=True)
	urlbase  = Column(String)				# URL base

	# Relationship to Packages (in this Channel)
	packages = relationship('Package', backref='channel', lazy='dynamic', cascade='all, delete, delete-orphan')

class Package(Base):
	__tablename__ = 'packages'
	__table_args__ = (
		UniqueConstraint('name', 'version', 'build_number', 'channel_id'),
		UniqueConstraint('recipe_hash', 'channel_id'),				# no two recipes can be identical on the same channel
		{'sqlite_autoincrement': True}
	)

	id           = Column(Integer, primary_key=True)

	name         = Column(String)
	version      = Column(String)
	build_number = Column(Integer)

	recipe_hash  = Column(String)

	# Relationship to Channel
	channel_id   = Column(Integer, ForeignKey('channels.id'))
#	channel      = relationship("Channel", backref=backref("packages", order_by=id, lazy='dynamic'))
	

# From http://stackoverflow.com/a/6078058/897575
#   model: class to query or create
#   kwargs: {member=value} dict of class members
def get_or_create(session, model, **kwargs):
	instance = session.query(model).filter_by(**kwargs).first()
	if instance:
		return instance
	else:
		instance = model(**kwargs)
		session.add(instance)
		session.commit()
		return instance

class ReleaseDB(object):
	server   = None
	channel  = None
	platform = None

	_db = None		# The loaded database (dict of dicts)

	def __init__(self):
		self._db = {}

		# Detect the platform (FIXME: there has to be a cleaner way to do this;
		# see how Conda deduces the platform string internally)
		import platform
		bits = platform.architecture()[0][:2]	# 64 or 32
		plat = 'osx' if sys.platform == 'darwin' else 'linux'
		self.platform = "%s-%s" % (plat, bits)

		# open the database, ensure the tables are defined
		dbfn = os.path.join('releasedb', self.platform, 'db.sqlite')
		try:
			os.makedirs(os.path.dirname(dbfn))
		except OSError:
			pass				# dir already exists

		##engine = sqlalchemy.create_engine('sqlite:///:memory:', echo=True)
		engine = sqlalchemy.create_engine('sqlite:///%s' % dbfn, echo=False)
		Base.metadata.create_all(engine)

		# create a session
		self._session = sessionmaker(bind=engine)()

	def hash_filelist(self, filelist, ignore_prefix='', open=open):
		import hashlib
		m = hashlib.sha1()

		if False:
			# Echo the output to the screen
			def update(m, s):
				sys.stdout.write("%s" % s)
				return m.update(s)
		else:
			def update(m, s): return m.update(s)

		for fn in sorted(filelist):
			# Ignore all files that *don't* end in the following suffixes
			suffixes = [ '.patch', '.yaml', '.patch', '.diff', '.sh' ]	# FIXME: make this configurable somehow
			for suffix in suffixes:
				if fn.endswith(suffix):
					break
			else:
				continue

			mm = hashlib.sha1()
			with open(fn) as fp:
				rel_fn = fn[len(ignore_prefix):]

				# Special handling of some files:
				if rel_fn == 'meta.yaml':
					# remove build number from the meta.yaml file
					# build:
					#   number: 0
					state = 0	# 0: scan for build, 1: scan for number: 2: pass through the rest of the file
					for line in fp:
						if state == 0 and line == 'build:\n':
							state = 1
						elif state == 1 and line.strip().startswith('number:'):
							line = ''	# don't write out the 
							state = 2
						elif state == 1 and not line.strip():
							state = 2	# didn't have an explicit number:

						mm.update(line)
				else:
					# Just add the file contents
					mm.update(fp.read())

			# Update the list hash
			update(m, "%s  %s\n" % (mm.hexdigest(), rel_fn))

		return m.hexdigest()

	def reindex(self, channels):
		# Reindex the channels
		cids = []
		for urlbase in channels:
			channel = get_or_create(self._session, Channel, urlbase=urlbase)
			self.reindex_channel(channel)
			cids.append(channel.id)

		# Delete information for any channel that wasn't in the list above
		for channel in self._session.query(Channel).filter(~Channel.id.in_(cids)).all():
			print "Removing channel %s... " % channel.urlbase,
			self._session.delete(channel)
			print "done."

		self._session.commit()

	def reindex_channel(self, channel):
		print "updating package cache for server %s, platform %s (this may take a while)..." % (channel.urlbase, self.platform)

		urlbase = '%s%s/' % (channel.urlbase, self.platform)

		# Fetch repodate.json using requests
		url = urlbase + 'repodata.json'
		r = requests.get(url)
		r.raise_for_status()
		repodata = r.json()

		# Fetch each package, extract and hash its recipe
		import tarfile
		for package, pkginfo in repodata[u'packages'].iteritems():
			name, version = [ pkginfo[s].encode('utf-8') for s in ['name', 'version'] ]
			build_number = pkginfo['build_number']

			# Skip if we already know about this package
			if channel.packages.filter(Package.name == name, Package.version == version, Package.build_number == build_number).count():
				print "already know about %s, skipping." % package
				continue

			# See if we know about this package in other channels; just copy the info if we do
			pkg = self._session.query(Package).filter_by(name=name, version=version, build_number=build_number).first()
			if pkg is not None:
				make_transient(pkg)
				pkg.id = None
				pkg.channel_id = channel.id
				self._session.add(pkg)
				continue

			pkgurl = urlbase + package
			_, suffix = os.path.splitext(pkgurl)

			# Download the package
			with tempfile.NamedTemporaryFile(suffix=suffix) as fp:
				print os.path.basename(pkgurl)
				download_url(pkgurl, fp)

				# Extract the recipe
				with tarfile.open(fp.name) as tf:
					prefix = 'info/recipe/'

					all = tf.getnames()
					info = [ fn for fn in all if fn.startswith(prefix) ]

					# hash all files in info/recipe/
					import contextlib
					hash = self.hash_filelist(info, prefix, open=lambda fn: contextlib.closing(tf.extractfile(fn)))

					# add to the database
					pkg = Package(name=name, version=version, build_number=build_number, recipe_hash=hash)
					channel.packages.append(pkg)

					try:
						ctr += 1
						if ctr == -1: break
					except:
						ctr = 0

			# write out the new database
			self._session.commit()
		self._session.commit()

	def hash_recipe(self, recipe_dir):
		# Compute recipe hash for files in recipe_dir

		# Get all files (incl. those in directories) and sort them
		def listfiles(dir):
			for root, directories, filenames in os.walk(dir):
				for filename in filenames: 
					yield os.path.join(root, filename)

		filelist = list(listfiles(recipe_dir))
		prefix = recipe_dir if recipe_dir.endswith('/') else recipe_dir + '/'

		hash = self.hash_filelist(filelist, prefix)
		return hash

	def get_next_buildnum(self, name, version):
		from sqlalchemy.sql import func
		max = self._session.query(func.max(Package.build_number)).filter(Package.name == name, Package.version == version).scalar()
		return max + 1 if max is not None else 0

	def __getitem__(self, key):
		# Return buildnum for (name, version, recipe_hash) if in the database
		name, version, recipe_hash = key
		return self._session.query(Package).filter_by(name=name, version=version, recipe_hash=recipe_hash).first().build_number


def report_progress(product, verstr = None):
	if verstr is not None:
		print "%s-%s...  " % (product, verstr),
	else:
		print "%s...  " % product,
	sys.stdout.flush()

def eups_to_conda_version(product, eups_version):
	# Convert EUPS version string to Conda-compatible pieces
	#
	# Conda version has three parts:
	#	version number: a version number that should be PEP 386 compliant (though Conda's impl. is buggy)
	#	build string: not used in version comparison, can be anything
	#	build number: if two versions are equal, build number is used to break the tie
	# Conda also doesn't like '+' nor '-' in versions
	#  Furthermore, it parses the version itself as described in the regex at:
	#     https://github.com/conda/conda/blob/master/conda/verlib.py
	#  which should be PEP 386 compliant (and very limited). We do our best here to fit into that straitjacket.

	# hardcoded for now. This should be incremented on a case-by-case basis to
	# push fixes that are Conda-build related
	buildnum = 0

	# Split into version + eups build number ("plusver")
	if '+' in eups_version:
		raw_version, plusver = eups_version.split('+')
		plusver = int(plusver)
	else:
		raw_version, plusver = eups_version, 0

	# Parse EUPS version:
	# Possibilities to detect:
	#	<vername>-<tagdist>-g<sha1>		-> (<vername>.<tagdist>, <plusver>_<sha1>, <buildnum>)
	#          <vername> can be <version>.lsst<N>	->   <vername>.<N>
	#	<branch>-g<sha1>			-> (<branch>_g<sha1>, <plusver>_<sha1>, <buildnum>)
	#	<something_completely_different>	-> (<something_completely_different>, '', <buildnum>)
	#

	def parse_full_version(version):	
		match = re.match('^([^-]+)-([0-9]+)-g([0-9a-z]+)$', version)
		if not match: return None, None

		vername, tagdist, sha1  = match.groups()

		# handle 1.2.3.lsst5 --> 1.2.3.5
		fixed_ver, _ = parse_lsst_patchlevel(vername)
		if fixed_ver is not None:
			vername = fixed_ver

		return "%s.%s" % (vername, tagdist), sha1

	def parse_lsst_patchlevel(version):
		# handle 1.2.3.lsst5 --> 1.2.3.5
		match = re.match(r'^(.*?).?lsst([0-9]+)$', version)
		if not match: return None, None

		true_ver, lsst_patch = match.groups()
		return "%s.%s" % (true_ver, lsst_patch), ''

	def parse_branch_sha1(version):
		match = re.match('^([^-]+)-g([0-9a-z]+)$', version)
		if not match: return None, None

		branch, sha1 = match.groups()
		return "%s_g%s" % (branch, sha1), sha1

	def parse_default(version):
		return version, ''

	parsers  = special_version_parsers.get(product, [])
	parsers += [ parse_full_version, parse_lsst_patchlevel, parse_branch_sha1, parse_default ]
	for parser in parsers:
		version, build_string_prefix = parser(raw_version)
		if version is not None:
			build_string = '%s_%s' % (build_string_prefix, buildnum) if build_string_prefix else str(buildnum)
			break

	# Heuristic for converting the (unnaturally) large LSST version numbers
	# to something more apropriate (i.e. 10.* -> 0.10.*, etc.).
	if re.match(r'^1[0-9]\.[0-9]+.*$', version):
		version = "0." + version

	# add plusver to version as .0000
	if plusver:
		version += ".%04d" % int(plusver)

	# remove any remaining '-'
	if '-' in version:
		version = version.replace('-', '_')

	return version, build_string, buildnum

def conda_version_spec(conda_name):
	pi = products[conda_name]
	if pi.version is not None:
		return "%s %s %s" % (conda_name, pi.version, pi.build_string)
	else:
		return conda_name

def create_yaml_list(elems, SEP='\n    - '):
	return (SEP + SEP.join(elems)) if elems else ''

def create_deps_string(deps, SEP='\n    - '):
	return create_yaml_list([dep.lower() for dep in deps], SEP)

def fill_out_template(dest_file, template_file, **variables):
	# fill out a template file
	with open(os.path.join('templates', template_file)) as fp:
		template = fp.read()

	text = template % variables

	with open(dest_file, 'w') as fp:
		fp.write(text)

def prepare_patches(product, dir):
	patchdir = os.path.join('patches', product)
	if not os.path.isdir(patchdir):
		return ''

	patch_files = glob.glob(os.path.join(patchdir, '*.patch'))

	for patchfn in patch_files:
		shutil.copy2(patchfn, dir)
	
	# convert to meta.yaml string
	patchlist = [ os.path.basename(p) for p in patch_files ]
	patches = '  patches:' + create_yaml_list(patchlist)
	return patches

def gen_conda_package(product, sha, eups_version, eups_deps):
	# What do we call this product in conda?
	conda_name = conda_name_for(product)

	# convert to conda version
	version, build_string, buildnum = eups_to_conda_version(product, eups_version)

	# write out a progress message
	report_progress(conda_name, "%s-%s" % (version, build_string))

	#
	# process dependencies
	#
	eups_deps = set(eups_deps)
	if eups_deps & internal_products:	# if we have any of the internal dependencies, make sure we depend on legacy_config where their .cfg and .table files are
		eups_deps.add('legacy_configs')
	eups_deps -= skip_products					# skip unwanted dependencies
	deps =  [ conda_name_for(prod) for prod in eups_deps ]		# transform to Anaconda product names
	deps += add_missing_deps(conda_name, output_dir)		# manually add any missing dependencies

	# flatten dependencies to work around a Conda bug:
	# https://github.com/conda/conda/issues/918
	def flatten_deps(deps, seen=None):
		if seen is None:
			seen = set()

		fdeps = set(deps)
		for dep in deps:
			if dep not in seen:
				try:
					pi = products[dep]
				except KeyError:
					pass
				else:
					fdeps |= flatten_deps(pi.deps, seen)
				seen.add(dep)
		return fdeps
	deps = sorted(flatten_deps(deps))

	# Where is the source?
	giturl = 'https://github.com/LSST/%s' % (product)

	#
	# Create the Conda packaging spec files
	#
	dir = os.path.join(output_dir, conda_name)
	os.makedirs(dir)

	deps = [ conda_version_spec(p) if p in products else p for p in deps ]
	reqstr = create_deps_string(deps)

	patches = prepare_patches(product, dir)

	fill_out_template(os.path.join(dir, 'meta.yaml'), 'meta.yaml.template',
		productNameLowercase = conda_name.lower(),
		version = version,
		buildnum = buildnum,
		string = build_string,
		gitrev = sha,
		giturl = giturl,
		build_req = reqstr,
		run_req = reqstr,
		patches = patches,
	)

	# build.sh (TBD: use exact eups versions, instead of -r .)
	setups = []
	SEP = 'setup '
	setups = SEP + ('\n'+SEP).join(setups) if setups else ''

	fill_out_template(os.path.join(dir, 'build.sh'), 'build.sh.template',
		setups = setups,
		eups_tags = ' '.join(eups_tags)
	)

	# pre-link.sh (to add the global tags)
	fill_out_template(os.path.join(dir, 'pre-link.sh'), 'pre-link.sh.template',
		product = product,
	)

	# record we've seen this product
	products[conda_name] = ProductInfo(conda_name, version, build_string, buildnum, product, eups_version, deps)


##################################
# PyPi dependencies support code
#

def conda_package_exists(conda_name):
	ret = subprocess.check_output('conda search -c defaults --override-channels -f --json %s' % (conda_name), shell=True).strip()
	return ret != "{}"

def gen_pypi_package(name, products, workdir):
	tmpdir = os.path.join(workdir, '_pypi')
	os.makedirs(tmpdir)

	# generate the packages
	retcode = subprocess.call('conda skeleton pypi %(name)s --recursive --output-dir %(pypi)s > %(pypi)s/output.log' % { 'name': name, 'pypi' : tmpdir }, shell=True)
	if retcode:
		raise Exception("conda skeleton returned %d" % retcode)

	# conda skeleton doesn't properly detect some pypi dependencies
	deps = add_missing_deps(name, tmpdir)
	if deps:
		# patch the generated meta.yaml file to add the missing dependenceis
		build_req = create_deps_string(deps)
		run_req   = create_deps_string(deps)

		metafn = os.path.join(tmpdir, name, 'meta.yaml')
		with open(metafn) as fp:
			meta = fp.read()

		import re
		meta = re.sub(r'(^requirements:\n  build:)',  r'\1' + build_req, meta, count=1, flags=re.MULTILINE)
		meta = re.sub(r'(^requirements:\n.*^  run:)', r'\1' +   run_req, meta, count=1, flags=re.MULTILINE | re.DOTALL)
		
		with open(metafn, "w") as fp:
			fp.write(meta)

	# see what was generated
	#with open(os.path.join(tmpdir, 'output.log')) as fp:
	#	packages = [ line.split()[-1] for line in fp if line.startswith("Writing recipe for ") ]

	# move into output directory any generated packages that aren't already there
	for package in os.listdir(tmpdir):
		src  = os.path.join(tmpdir, package)
		if not os.path.isdir(src):
			continue

		dest = os.path.join(workdir, package)
		if not os.path.isdir(dest) and not conda_package_exists(package):
			#print "MOVING: ", src, dest
			os.rename(src, dest)

			if package not in products:
				products[package] = ProductInfo(package, None, None, None, None, None, [])

			if workdir == output_dir:
				report_progress(package)

	# delete what remains
	shutil.rmtree(tmpdir)

def add_missing_deps(conda_name, workdir):
	# inject missing dependencies, creating new conda packages if needed
	# returns Conda package names
	deps = []
	for kind, dep in missing_deps.get(conda_name, []):
		#print conda_name, ':', kind, dep
		{
			'pypi': gen_pypi_package,
			'conda': lambda dep, products, workdir: None
		}[kind](dep, products, workdir)
		deps.append(dep)

	return deps

def load_manifest(fn):
	# loads a manifest created by lsst_build
	with open(fn) as fp:
		lines = fp.read().split('\n')

	lines = lines[2:]
	for line in lines:
		line = line.strip()
		if not line:
			continue
		if line.startswith('#'):
			continue

		try:
			(product, sha, version, deps) = line.split()
			deps = deps.split(',')
		except ValueError:
			(product, sha, version) = line.split()
			deps = []

		yield (product, sha, version, deps)


def test_release_db():
	from requests_file import FileAdapter	# run 'pip install requests_file' if this fails
	global requests

	s = requests.Session()
	s.mount('file://', FileAdapter())
	requests = s

	db = ReleaseDB()

	name, version = "eups", "1.5.9_1"
	dir = 'recipes/static/eups'
	hash = db.hash_recipe(dir)
	print "hash for %s: %s" % (dir, hash)

	channels = [
		'file:///Users/mjuric/projects/lsst_packaging_conda/miniconda/conda-bld/',
		'http://conda.anaconda.org/lsst/channel/dev/'
	]

	db.reindex(channels)

	print "next buildnum:", db.get_next_buildnum(name, version)
	print "hash lookup: ", db[name, version, hash]

if __name__ == "__main__":
	test_release_db()
	exit()

	import argparse
	parser = argparse.ArgumentParser()
	parser.add_argument("manifest", help="lsst_build-generated manifest file from which to read the package list and their versions. The format is the same as that found in https://github.com/lsst/versiondb/tree/master/manifests", type=str)
	parser.add_argument("products", help="the top-level products; Conda recipes will be generated for these and all their dependencies.", type=str, nargs='+')
	args = parser.parse_args()

	# Load the manifest
	products = {}
	for (product, sha, version, deps) in load_manifest(args.manifest):
		products[product] = (product, sha, version, deps)

	# Extract the products of interest (and their dependencies)
	manifest = OrderedDict()
	def bottom_up_add_to_manifest(product):
		(product, sha, version, deps) = products[product]
		for dep in deps:
			bottom_up_add_to_manifest(dep)
		if product not in manifest:
			manifest[product] = products[product]

	top_level = args.products
	for product in top_level:
		bottom_up_add_to_manifest(product)

	# Generate conda package files and build driver script
	shutil.rmtree(output_dir, ignore_errors=True)
	os.makedirs(output_dir)
	products = OrderedDict()
	for (product, sha, version, deps) in manifest.itervalues():
		if product in internal_products: continue
		if product in skip_products: continue

		# override gitrevs (these are temporary hacks/fixes; they should go away when those branches are merged)
		sha = override_gitrev.get(product, sha)

		gen_conda_package(product, sha, version, deps)
	print "done."

	#
	# write out the rebuild script
	#
	comment = ''
	rebuilds = []
	for pi in products.itervalues():
		if pi.product == 'sims_photUtils':
			comment = ''
		conda_version = "%s-%s" % (pi.version, pi.build_string)
		rebuilds.append(comment + "rebuild %s %s %s %s" % (pi.conda_name, conda_version, pi.product, pi.eups_version))

	fill_out_template(os.path.join(output_dir, 'rebuild.sh'), 'rebuild.sh.template',
		output_dir = output_dir,
		rebuilds = '\n'.join(rebuilds)
		)

	print ""
	print "Generation completed; The recipes are in %s directory." % (output_dir)
	print "Run 'bash %s/rebuild.sh' to build them." % (output_dir)
